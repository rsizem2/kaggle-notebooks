{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6802a0",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "These techniques are adapted from the ones covered in the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "053251f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for testing changes to this notebook quickly\n",
    "FOLD_SEED = 0\n",
    "NUM_FOLDS = 3\n",
    "EARLY_STOP = 50\n",
    "TRIALS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfef0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from functools import partial \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from category_encoders import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Models and Hyperparameter Tuning\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d035a6",
   "metadata": {},
   "source": [
    "# Create Folds and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96978af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Remove rows with missing target\n",
    "train.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "# Clean data, static transformations\n",
    "def clean_data(*data):\n",
    "    for df in data:\n",
    "        # fix typos to match documentation\n",
    "        df['MSZoning'] =  df['MSZoning'].replace({'C (all)': 'C'})\n",
    "        df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\":\"BrkComm\",\"Wd Shng\": \"WdShing\"})\n",
    "        df['Neighborhood'] = df['Neighborhood'].replace({'NAmes':'Names'})\n",
    "\n",
    "        # Some values of GarageYrBlt are corrupt, replace them with YearBuilt\n",
    "        df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n",
    "        \n",
    "        # optional feature: A - agriculture, C - commercial, R - residential, I - industrial\n",
    "        df[\"MSClass\"] = df['MSZoning'].map({'A': 'A','C': 'C',\"FV\": 'R','I': 'I',\n",
    "                                            \"RH\": 'R',\"RL\": 'R',\"RP\": 'R',\"RM\": 'R', np.nan:np.nan})\n",
    "    return data\n",
    "    \n",
    "train, test = clean_data(train, test)\n",
    "\n",
    "# List of categorical/numerical columns\n",
    "columns = [col for col in test.columns if col not in [\"Id\",\"MSClass\"]]\n",
    "object_cols = [col for col in columns if train[col].dtype == \"object\"]\n",
    "number_cols = [col for col in columns if train[col].dtype != \"object\"]\n",
    "\n",
    "# Define bins\n",
    "binner = KBinsDiscretizer(n_bins = 45, encode = 'ordinal', strategy = 'uniform')\n",
    "y_bins = binner.fit_transform(pd.DataFrame(data=train['SalePrice']))\n",
    "\n",
    "# Define folds\n",
    "train[\"kfold\"] = -1\n",
    "kf = StratifiedKFold(NUM_FOLDS, shuffle = True, random_state = FOLD_SEED) \n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_bins)):\n",
    "    train.loc[valid_idx,\"kfold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caad2647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, X_valid, X_test):\n",
    "    \n",
    "    # 1. find columns in X_train with high ratio of NAs\n",
    "    temp = pd.DataFrame(X_train.isnull().sum().sort_values(ascending = False), \n",
    "                        columns = ['NAs'])\n",
    "    temp['ratio'] = temp['NAs'].apply(lambda x: round(x/X_train.shape[0],2))\n",
    "    cols = list(temp[temp['ratio'] >= 0.8].index.values)\n",
    "    \n",
    "    # 2. drop the offending columns\n",
    "    X_train.drop(cols, inplace = True, axis = 1)\n",
    "    X_valid.drop(cols, inplace = True, axis = 1)\n",
    "    X_test.drop(cols, inplace = True, axis = 1)\n",
    "    \n",
    "    # 3. impute numerical data\n",
    "    columns = [col for col in X_train.columns if X_train[col].dtype != \"object\"]\n",
    "    if columns:\n",
    "        imputer = SimpleImputer(strategy='median')\n",
    "        X_train[columns] = imputer.fit_transform(X_train[columns])\n",
    "        X_valid[columns] = imputer.transform(X_valid[columns])\n",
    "        X_test[columns] = imputer.transform(X_test[columns])\n",
    "    \n",
    "    # 4. impute categorical data\n",
    "    columns = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "    if columns:\n",
    "        imputer = SimpleImputer(strategy='constant', fill_value = 'None')\n",
    "        X_train[columns] = imputer.fit_transform(X_train[columns])\n",
    "        X_valid[columns] = imputer.transform(X_valid[columns])\n",
    "        X_test[columns] = imputer.transform(X_test[columns])\n",
    "    \n",
    "    # 5. encode 1-10 ratings\n",
    "    cols = [\"OverallQual\",\"OverallCond\"]\n",
    "    cols = [x for x in cols if x in X_train.columns]\n",
    "    ratings = {float(a):b for b,a in enumerate(range(1,11))}\n",
    "    mapping = [{'col':x, 'mapping': ratings} for x in cols]\n",
    "    \n",
    "    encoder = OrdinalEncoder(cols = cols, mapping = mapping, handle_missing = 'return_nan')\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    # 6. encode Poor, Fair, Avg, Good, Ex ratings\n",
    "    cols = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"HeatingQC\", \"KitchenQual\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",'PoolQC']\n",
    "    cols = [x for x in cols if x in X_train.columns]\n",
    "    ratings = {\"Po\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4}\n",
    "    mapping = [{'col':x, 'mapping': ratings} for x in cols]\n",
    "    \n",
    "    encoder = OrdinalEncoder(cols = cols, mapping = mapping, handle_missing = 'return_nan')\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    # 7. encode remaining ordinal data\n",
    "    cols = [\"LotShape\",\"LandSlope\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\n",
    "    \"Functional\",\"GarageFinish\",\"PavedDrive\",\"Utilities\",\"CentralAir\",\"Electrical\",\n",
    "    \"Fence\"]\n",
    "    cols = [x for x in cols if x in X_train.columns]\n",
    "    mapping = [{'col':\"LotShape\",\n",
    "                'mapping': {\"Reg\":0, \"IR1\":1, \"IR2\":2, \"IR3\":3}},\n",
    "               {'col':\"LandSlope\",\n",
    "                'mapping': {\"Sev\":0, \"Mod\":1, \"Gtl\":2}},\n",
    "               {'col':\"BsmtExposure\",\n",
    "                'mapping': {\"No\":0, \"Mn\":1, \"Av\":2, \"Gd\":3}},\n",
    "               {'col':\"BsmtFinType1\",\n",
    "                'mapping': {\"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}},\n",
    "               {'col':\"BsmtFinType2\",\n",
    "                'mapping': {\"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}},\n",
    "               {'col':\"Functional\",\n",
    "                'mapping': {\"Sal\":0, \"Sev\":1, \"Maj1\":2, \"Maj2\":3, \"Mod\":4, \"Min2\":5, \"Min1\":6, \"Typ\":7}},\n",
    "               {'col':\"GarageFinish\",\n",
    "                'mapping': {\"Unf\":0, \"RFn\":1, \"Fin\":2}},\n",
    "               {'col':\"PavedDrive\",\n",
    "                'mapping': {\"N\":0, \"P\":1, \"Y\":2}},\n",
    "               {'col':\"Utilities\",\n",
    "                'mapping': {\"NoSeWa\":0, \"NoSewr\":1, \"AllPub\":2}},\n",
    "               {'col':\"CentralAir\",\n",
    "                'mapping': {\"N\":0, \"Y\":1}},\n",
    "               {'col':\"Electrical\",\n",
    "                'mapping': {\"Mix\":0, \"FuseP\":1, \"FuseF\":2, \"FuseA\":3, \"SBrkr\":4}},\n",
    "               {'col':\"Fence\",\n",
    "                'mapping': {\"MnWw\":0, \"GdWo\":1, \"MnPrv\":2, \"GdPrv\":3}}]\n",
    "    mapping = [x for x in mapping if x['col'] in X_train.columns]\n",
    "    \n",
    "    encoder = OrdinalEncoder(cols = cols, mapping = mapping, handle_missing = 'return_nan')\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    # 8. encode remaining columns\n",
    "    columns = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "    encoder = OrdinalEncoder(cols = columns, handle_missing = 'return_nan')\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "        \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357ccc3d",
   "metadata": {},
   "source": [
    "# Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a775e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_xgboost(transforms = [], params = {}, cols = columns, verbose = True):\n",
    "    start = time.time()\n",
    "    \n",
    "    X = train.copy()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    transforms = [preprocessing] + transforms\n",
    "    \n",
    "    for i in range(NUM_FOLDS):\n",
    "        X_train = X[X.kfold != i][cols].copy()\n",
    "        X_valid = X[X.kfold == i][cols].copy()\n",
    "        y_train = X[X.kfold != i]['SalePrice'].copy()\n",
    "        y_valid = X[X.kfold == i]['SalePrice'].copy()\n",
    "        X_test = test[cols].copy()\n",
    "        \n",
    "        # loop for applying the transformations\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test, y_train = y_train)\n",
    "            except:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test)\n",
    "        \n",
    "        model = XGBRegressor(**{**{'random_state':0, 'n_estimators': 3000},**params})\n",
    "        model.fit(X_train, y_train,\n",
    "                  verbose = False,\n",
    "                  eval_set = [(X_valid, y_valid)],\n",
    "                  eval_metric = \"mae\",\n",
    "                  early_stopping_rounds = EARLY_STOP)\n",
    "\n",
    "        preds = model.predict(X_valid)\n",
    "        scores[i] = mean_absolute_error(y_valid, preds)\n",
    "    end = time.time()\n",
    "    if verbose:\n",
    "        print(\"XGBoost  (3-fold Avg):\", \n",
    "              round(scores.mean(), 4))\n",
    "        print(\"XGBoost  (3-fold Max):\", \n",
    "              round(scores.max(), 4), \"\\t\",\n",
    "              str(round(end-start, 3))+\"s\")\n",
    "\n",
    "    return round(scores.mean(), 4), round(scores.max(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18212808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_lightgbm(transforms = [], params = {}, cols = columns, verbose = True):\n",
    "    start = time.time()\n",
    "    \n",
    "    X = train.copy()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    transforms = [preprocessing] + transforms\n",
    "    \n",
    "    for i in range(NUM_FOLDS):\n",
    "        X_train = X[X.kfold != i][cols].copy()\n",
    "        X_valid = X[X.kfold == i][cols].copy()\n",
    "        y_train = X[X.kfold != i]['SalePrice'].copy()\n",
    "        y_valid = X[X.kfold == i]['SalePrice'].copy()\n",
    "        X_test = test[cols].copy()\n",
    "        \n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test, y_train = y_train)\n",
    "            except:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test)\n",
    "                \n",
    "        cat_cols = [x for x in X_train.columns if x in object_cols]\n",
    "                \n",
    "        model = LGBMRegressor(**{**{'random_state':0, 'n_estimators': 3000},**params})\n",
    "        model.fit(X_train, y_train,\n",
    "                  verbose = False,\n",
    "                  eval_set = [(X_valid, y_valid)],\n",
    "                  eval_metric = \"mae\",\n",
    "                  categorical_feature = cat_cols,\n",
    "                  early_stopping_rounds = EARLY_STOP)\n",
    "\n",
    "        valid_preds = model.predict(X_valid)\n",
    "        scores[i] = mean_absolute_error(y_valid, valid_preds)\n",
    "    end = time.time()\n",
    "    if verbose:\n",
    "        print(\"LightGBM (3-fold Avg):\", \n",
    "              round(scores.mean(), 4))\n",
    "        print(\"LightGBM (3-fold Max):\", \n",
    "              round(scores.max(), 4), \"\\t\",\n",
    "              str(round(end-start, 3))+\"s\")\n",
    "\n",
    "    return round(scores.mean(), 4), round(scores.max(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f58755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_catboost(transforms = [], params = {}, cols = columns, verbose = True):\n",
    "    start = time.time()\n",
    "    \n",
    "    X = train.copy()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    transforms = [preprocessing] + transforms\n",
    "    \n",
    "    for i in range(NUM_FOLDS):\n",
    "        X_train = X[X.kfold != i][cols].copy()\n",
    "        X_valid = X[X.kfold == i][cols].copy()\n",
    "        y_train = X[X.kfold != i]['SalePrice'].copy()\n",
    "        y_valid = X[X.kfold == i]['SalePrice'].copy()\n",
    "        X_test = test[cols].copy()\n",
    "        \n",
    "        # loop for applying the transformations\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test, y_train = y_train)\n",
    "            except:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test)\n",
    "        \n",
    "        model = CatBoostRegressor(**{**{'random_state':0, \n",
    "                                        'n_estimators': 3000,\n",
    "                                        'eval_metric':\"MAE\",\n",
    "                                        'early_stopping_rounds': EARLY_STOP,\n",
    "                                        'verbose': False}, **params})\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set = (X_valid, y_valid),\n",
    "                  use_best_model=True)\n",
    "\n",
    "        valid_preds = model.predict(X_valid)\n",
    "        scores[i] = mean_absolute_error(y_valid, valid_preds)\n",
    "    end = time.time()\n",
    "    if verbose:\n",
    "        print(\"CatBoost (3-fold Avg):\", \n",
    "              round(scores.mean(), 4))\n",
    "        print(\"CatBoost (3-fold Max):\", \n",
    "              round(scores.max(), 4), \"\\t\",\n",
    "              str(round(end-start, 3))+\"s\")\n",
    "\n",
    "    return round(scores.mean(), 4), round(scores.max(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb758c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "XGBoost  (3-fold Avg): 17750.9292\n",
      "XGBoost  (3-fold Max): 18307.5135 \t 1.638s\n",
      "LightGBM (3-fold Avg): 16623.8913\n",
      "LightGBM (3-fold Max): 17442.5085 \t 1.565s\n",
      "CatBoost (3-fold Avg): 15306.813\n",
      "CatBoost (3-fold Max): 15716.4108 \t 8.733s\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n"
     ]
    }
   ],
   "source": [
    "def get_baseline():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost()\n",
    "    lgbm_avg, lgbm_max = score_lightgbm()\n",
    "    cat_avg, cat_max = score_catboost()\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    \n",
    "    return round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4), round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4)\n",
    "    \n",
    "BASELINE_AVG, BASELINE_MAX = get_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9744a1d",
   "metadata": {},
   "source": [
    "# Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e7454d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Drop Uninformative\n",
      "\n",
      "XGBoost  (3-fold Avg): 17553.6623\n",
      "XGBoost  (3-fold Max): 18282.7652 \t 2.573s\n",
      "LightGBM (3-fold Avg): 16586.4857\n",
      "LightGBM (3-fold Max): 17202.7019 \t 2.482s\n",
      "CatBoost (3-fold Avg): 15185.9457\n",
      "CatBoost (3-fold Max): 15747.9589 \t 8.829s\n",
      "\n",
      "Overall (Avg):         16442.0312\n",
      "Overall (Max):         17553.6623\n"
     ]
    }
   ],
   "source": [
    "def remove_uninformative(X_train, X_valid, X_test, y_train, verbose = False):\n",
    "    \n",
    "    # 1. Determine uninformative columns\n",
    "    scores =  mutual_info_regression(X_train, y_train)\n",
    "    cols = [x for i, x in enumerate(X_train.columns) if scores[i] == 0]\n",
    "    \n",
    "    # 2. Drop the uninformative columns\n",
    "    X_train.drop(cols, axis = 1, inplace = True)\n",
    "    X_valid.drop(cols, axis = 1, inplace = True)\n",
    "    X_test.drop(cols, axis = 1, inplace = True)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Dropped columns:\", *cols)\n",
    "    \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def test_uninformative():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    print(\"\\nDrop Uninformative\\n\")\n",
    "    transforms = [remove_uninformative]\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    \n",
    "test_uninformative()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8175e977",
   "metadata": {},
   "source": [
    "# Mathematical Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7513a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Mathematical Transformations\n",
      "\n",
      "XGBoost  (3-fold Avg): 17431.0216\n",
      "XGBoost  (3-fold Max): 17721.1326 \t 1.664s\n",
      "LightGBM (3-fold Avg): 16562.6011\n",
      "LightGBM (3-fold Max): 17653.9238 \t 1.632s\n",
      "CatBoost (3-fold Avg): 15109.5324\n",
      "CatBoost (3-fold Max): 15656.9734 \t 8.905s\n",
      "\n",
      "Overall (Avg):         16367.7184\n",
      "Overall (Max):         17431.0216\n"
     ]
    }
   ],
   "source": [
    "def mathematical_transformations(X_train, X_valid, X_test):\n",
    "    \n",
    "    X_train[\"LivLotRatio\"] = X_train[\"GrLivArea\"] / X_train[\"LotArea\"]\n",
    "    X_valid[\"LivLotRatio\"] = X_valid[\"GrLivArea\"] / X_valid[\"LotArea\"]\n",
    "    X_test[\"LivLotRatio\"] = X_test[\"GrLivArea\"] / X_test[\"LotArea\"]\n",
    "    \n",
    "    X_train[\"Spaciousness\"] = (X_train[\"1stFlrSF\"]+X_train[\"2ndFlrSF\"]) / X_train[\"TotRmsAbvGrd\"]\n",
    "    X_valid[\"Spaciousness\"] = (X_valid[\"1stFlrSF\"]+X_valid[\"2ndFlrSF\"]) / X_valid[\"TotRmsAbvGrd\"]\n",
    "    X_test[\"Spaciousness\"] = (X_test[\"1stFlrSF\"]+X_test[\"2ndFlrSF\"]) / X_test[\"TotRmsAbvGrd\"]\n",
    "    \n",
    "    X_train[\"TotalOutsideSF\"] = X_train[\"WoodDeckSF\"] + X_train[\"OpenPorchSF\"] + X_train[\"EnclosedPorch\"] + X_train[\"3SsnPorch\"] + X_train[\"ScreenPorch\"]\n",
    "    X_valid[\"TotalOutsideSF\"] = X_valid[\"WoodDeckSF\"] + X_valid[\"OpenPorchSF\"] + X_valid[\"EnclosedPorch\"] + X_valid[\"3SsnPorch\"] + X_valid[\"ScreenPorch\"]\n",
    "    X_test[\"TotalOutsideSF\"] = X_test[\"WoodDeckSF\"] + X_test[\"OpenPorchSF\"] + X_test[\"EnclosedPorch\"] + X_test[\"3SsnPorch\"] + X_test[\"ScreenPorch\"]\n",
    "    \n",
    "    X_train['TotalLot'] = X_train['LotFrontage'] + X_train['LotArea']\n",
    "    X_valid['TotalLot'] = X_valid['LotFrontage'] + X_valid['LotArea']\n",
    "    X_test['TotalLot'] = X_test['LotFrontage'] + X_test['LotArea']\n",
    "    \n",
    "    X_train['TotalBsmtFin'] = X_train['BsmtFinSF1'] + X_train['BsmtFinSF2']\n",
    "    X_valid['TotalBsmtFin'] = X_valid['BsmtFinSF1'] + X_valid['BsmtFinSF2']\n",
    "    X_test['TotalBsmtFin'] = X_test['BsmtFinSF1'] + X_test['BsmtFinSF2']\n",
    "    \n",
    "    X_train['TotalSF'] = X_train['TotalBsmtSF'] + X_train['2ndFlrSF'] + X_train['1stFlrSF']\n",
    "    X_valid['TotalSF'] = X_valid['TotalBsmtSF'] + X_valid['2ndFlrSF'] + X_valid['1stFlrSF']\n",
    "    X_test['TotalSF'] = X_test['TotalBsmtSF'] + X_test['2ndFlrSF'] + X_test['1stFlrSF']\n",
    "    \n",
    "    X_train['TotalBath'] = X_train['FullBath'] + X_train['HalfBath'] * 0.5 + X_train['BsmtFullBath'] + X_train['BsmtHalfBath'] * 0.5\n",
    "    X_valid['TotalBath'] = X_valid['FullBath'] + X_valid['HalfBath'] * 0.5 + X_valid['BsmtFullBath'] + X_valid['BsmtHalfBath'] * 0.5\n",
    "    X_test['TotalBath'] = X_test['FullBath'] + X_test['HalfBath'] * 0.5 + X_test['BsmtFullBath'] + X_test['BsmtHalfBath'] * 0.5\n",
    "    \n",
    "    X_train['TotalPorch'] = X_train['OpenPorchSF'] + X_train['EnclosedPorch'] + X_train['ScreenPorch'] + X_train['WoodDeckSF']\n",
    "    X_valid['TotalPorch'] = X_valid['OpenPorchSF'] + X_valid['EnclosedPorch'] + X_valid['ScreenPorch'] + X_valid['WoodDeckSF']\n",
    "    X_test['TotalPorch'] = X_test['OpenPorchSF'] + X_test['EnclosedPorch'] + X_test['ScreenPorch'] + X_test['WoodDeckSF']\n",
    "    \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def test_transformations():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    print(\"\\nMathematical Transformations\\n\")\n",
    "    transforms = [mathematical_transformations]\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    \n",
    "test_transformations()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cc7056",
   "metadata": {},
   "source": [
    "# Encoding Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebcfff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Interaction Term:\n",
      "\n",
      "XGBoost  (3-fold Avg): 17910.0346\n",
      "XGBoost  (3-fold Max): 18361.0122 \t 1.65s\n",
      "LightGBM (3-fold Avg): 16832.9489\n",
      "LightGBM (3-fold Max): 17885.9157 \t 1.521s\n",
      "CatBoost (3-fold Avg): 15236.8674\n",
      "CatBoost (3-fold Max): 15634.9627 \t 8.328s\n",
      "\n",
      "Overall (Avg):         16659.9503\n",
      "Overall (Max):         17910.0346\n"
     ]
    }
   ],
   "source": [
    "def encode_interaction(X_train, X_valid, X_test, cat_col = \"BldgType\", num_col = \"GrLivArea\"):\n",
    "\n",
    "    X_1 = pd.get_dummies(X_train[cat_col], prefix=cat_col)\n",
    "    X_2 = pd.get_dummies(X_valid[cat_col], prefix=cat_col)\n",
    "    X_3 = pd.get_dummies(X_test[cat_col], prefix=cat_col)\n",
    "        \n",
    "    for col in X_1.columns:\n",
    "        X_train[col+\"_\"+num_col] = X_1[col]*X_train[num_col]\n",
    "        X_valid[col+\"_\"+num_col] = X_2[col]*X_valid[num_col]\n",
    "        X_test[col+\"_\"+num_col] = X_3[col]*X_test[num_col]\n",
    "    \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def test_interaction():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)  \n",
    "    \n",
    "    transforms = [encode_interaction]\n",
    "    print(\"\\nInteraction Term:\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    \n",
    "test_interaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f51179",
   "metadata": {},
   "source": [
    "# Count Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dac1c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Count Feature:\n",
      "\n",
      "XGBoost  (3-fold Avg): 17914.7202\n",
      "XGBoost  (3-fold Max): 18285.0698 \t 1.867s\n",
      "LightGBM (3-fold Avg): 16680.4994\n",
      "LightGBM (3-fold Max): 17505.6324 \t 1.574s\n",
      "CatBoost (3-fold Avg): 15193.0855\n",
      "CatBoost (3-fold Max): 15788.8947 \t 9.847s\n",
      "\n",
      "Overall (Avg):         16596.1017\n",
      "Overall (Max):         17914.7202\n"
     ]
    }
   ],
   "source": [
    "def count_porch_types(X_train, X_valid, X_test):\n",
    "    \n",
    "    X_train[\"PorchTypes\"] = X_train[[\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]].gt(0).sum(axis=1)\n",
    "    X_valid[\"PorchTypes\"] = X_valid[[\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]].gt(0).sum(axis=1)\n",
    "    X_test[\"PorchTypes\"] = X_test[[\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]].gt(0).sum(axis=1)\n",
    "        \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def test_count_feature():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    transforms = [count_porch_types]\n",
    "    print(\"\\nCount Feature:\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "      \n",
    "test_count_feature()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111c1400",
   "metadata": {},
   "source": [
    "# Break Down Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4130b8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Break Down:\n",
      "\n",
      "XGBoost  (3-fold Avg): 17722.4002\n",
      "XGBoost  (3-fold Max): 18074.0544 \t 1.62s\n",
      "LightGBM (3-fold Avg): 16623.8913\n",
      "LightGBM (3-fold Max): 17442.5085 \t 1.513s\n",
      "CatBoost (3-fold Avg): 15181.7648\n",
      "CatBoost (3-fold Max): 15845.3079 \t 10.877s\n",
      "\n",
      "Overall (Avg):         16509.3521\n",
      "Overall (Max):         17722.4002\n"
     ]
    }
   ],
   "source": [
    "def test_breakdown():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    print(\"\\nBreak Down:\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(cols = columns + [\"MSClass\"])\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(cols = columns + [\"MSClass\"])\n",
    "    cat_avg, cat_max = score_catboost(cols = columns + [\"MSClass\"])\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "      \n",
    "    \n",
    "test_breakdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e46d85",
   "metadata": {},
   "source": [
    "# Grouped Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fe67aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Group Transformation:\n",
      "\n",
      "XGBoost  (3-fold Avg): 17974.0845\n",
      "XGBoost  (3-fold Max): 18608.1193 \t 1.569s\n",
      "LightGBM (3-fold Avg): 16718.6506\n",
      "LightGBM (3-fold Max): 17527.5949 \t 1.52s\n",
      "CatBoost (3-fold Avg): 15001.0547\n",
      "CatBoost (3-fold Max): 15451.3731 \t 8.001s\n",
      "\n",
      "Overall (Avg):         16564.5966\n",
      "Overall (Max):         17974.0845\n"
     ]
    }
   ],
   "source": [
    "def group_transformation(X_train, X_valid, X_test):\n",
    "    \n",
    "    X_train[\"MedNhbdLvArea\"] = X_train.groupby(\"Neighborhood\")[\"GrLivArea\"].transform('median')\n",
    "    \n",
    "    # we use the medians from the training data to impute the test data\n",
    "    mapping = dict()\n",
    "    for x,y in zip(X_train[\"MedNhbdLvArea\"].iteritems(), X_train['Neighborhood'].iteritems()):\n",
    "        _,median_area = x\n",
    "        _,nbhr = y\n",
    "        if nbhr not in mapping: mapping[nbhr] = median_area\n",
    "    \n",
    "    X_valid[\"MedNhbdLvArea\"] = X_valid['Neighborhood'].map(mapping)\n",
    "    X_test[\"MedNhbdLvArea\"] = X_test['Neighborhood'].map(mapping)\n",
    "    \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def test_group():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    transforms = [group_transformation]\n",
    "    print(\"\\nGroup Transformation:\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "      \n",
    "    \n",
    "test_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a619fa35",
   "metadata": {},
   "source": [
    "# Clustering (Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98d24e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Cluster Labels:\n",
      "\n",
      "XGBoost  (3-fold Avg): 17917.9605\n",
      "XGBoost  (3-fold Max): 18428.5234 \t 1.81s\n",
      "LightGBM (3-fold Avg): 16574.1062\n",
      "LightGBM (3-fold Max): 17495.0389 \t 1.671s\n",
      "CatBoost (3-fold Avg): 15351.4654\n",
      "CatBoost (3-fold Max): 15897.7019 \t 8.678s\n",
      "\n",
      "Overall (Avg):         16614.5107\n",
      "Overall (Max):         17917.9605\n"
     ]
    }
   ],
   "source": [
    "def generate_cluster_labels(X_train, X_valid, X_test, name = \"Area\", features = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF','GrLivArea']):\n",
    "    \n",
    "    # 1. normalize based on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_valid_scaled = scaler.transform(X_valid[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    \n",
    "    # 2. create cluster labels (use predict)\n",
    "    X_1, X_2 = pd.DataFrame(), pd.DataFrame()\n",
    "    kmeans = KMeans(n_clusters = 10, n_init = 10, random_state=0)\n",
    "    X_train[name + \"_Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    X_valid[name + \"_Cluster\"] = kmeans.predict(X_valid_scaled)\n",
    "    X_test[name + \"_Cluster\"] = kmeans.predict(X_test_scaled)\n",
    "         \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def test_cluster_labels():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    transforms = [generate_cluster_labels]\n",
    "    print(\"\\nCluster Labels:\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "     \n",
    "test_cluster_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90dc22",
   "metadata": {},
   "source": [
    "# Clustering (Distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddac2605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Cluster Distances:\n",
      "\n",
      "XGBoost  (3-fold Avg): 18070.1014\n",
      "XGBoost  (3-fold Max): 18889.9244 \t 1.903s\n",
      "LightGBM (3-fold Avg): 17106.0203\n",
      "LightGBM (3-fold Max): 17812.7573 \t 1.893s\n",
      "CatBoost (3-fold Avg): 15537.4439\n",
      "CatBoost (3-fold Max): 16187.6359 \t 10.731s\n",
      "\n",
      "Overall (Avg):         16904.5219\n",
      "Overall (Max):         18070.1014\n"
     ]
    }
   ],
   "source": [
    "def generate_cluster_distances(X_train, X_valid, X_test = None, name = \"Area\", features = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF','GrLivArea']):\n",
    "    \n",
    "    # 1. normalize based on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_valid_scaled = scaler.transform(X_valid[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    \n",
    "    # 2. generate cluster distances (use transform)\n",
    "    kmeans = KMeans(n_clusters = 10, n_init = 10, random_state=0)\n",
    "    X_cd = kmeans.fit_transform(X_scaled)\n",
    "    X_valid_cd = kmeans.transform(X_valid_scaled)\n",
    "    X_test_cd = kmeans.transform(X_test_scaled)\n",
    "    \n",
    "    # 3. column labels\n",
    "    X_cd = pd.DataFrame(X_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_cd.shape[1])])\n",
    "    X_valid_cd = pd.DataFrame(X_valid_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_valid_cd.shape[1])])\n",
    "    X_test_cd = pd.DataFrame(X_test_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_valid_cd.shape[1])])\n",
    "    \n",
    "    return X_train.join(X_cd), X_valid.join(X_valid_cd), X_test.join(X_test_cd)\n",
    "\n",
    "def test_cluster_distances():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    transforms = [generate_cluster_distances]\n",
    "    print(\"\\nCluster Distances:\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "     \n",
    "    \n",
    "test_cluster_distances()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629bd1b",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94933ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "PCA (1 components):\n",
      "\n",
      "XGBoost  (3-fold Avg): 17974.0797\n",
      "XGBoost  (3-fold Max): 18334.2837 \t 1.921s\n",
      "LightGBM (3-fold Avg): 16767.0964\n",
      "LightGBM (3-fold Max): 17324.6553 \t 1.704s\n",
      "CatBoost (3-fold Avg): 15433.9841\n",
      "CatBoost (3-fold Max): 15851.3625 \t 8.049s\n",
      "\n",
      "Overall (Avg):         16725.0534\n",
      "Overall (Max):         17974.0797\n",
      "\n",
      "PCA (2 components):\n",
      "\n",
      "XGBoost  (3-fold Avg): 18067.0729\n",
      "XGBoost  (3-fold Max): 18441.1984 \t 2.312s\n",
      "LightGBM (3-fold Avg): 16920.6678\n",
      "LightGBM (3-fold Max): 17892.484 \t 1.689s\n",
      "CatBoost (3-fold Avg): 15441.937\n",
      "CatBoost (3-fold Max): 16010.6142 \t 9.294s\n",
      "\n",
      "Overall (Avg):         16809.8926\n",
      "Overall (Max):         18067.0729\n",
      "\n",
      "PCA (3 components):\n",
      "\n",
      "XGBoost  (3-fold Avg): 18036.673\n",
      "XGBoost  (3-fold Max): 18392.6344 \t 2.074s\n",
      "LightGBM (3-fold Avg): 16716.2553\n",
      "LightGBM (3-fold Max): 17289.0693 \t 1.689s\n",
      "CatBoost (3-fold Avg): 15487.882\n",
      "CatBoost (3-fold Max): 16253.2187 \t 11.07s\n",
      "\n",
      "Overall (Avg):         16746.9368\n",
      "Overall (Max):         18036.673\n"
     ]
    }
   ],
   "source": [
    "# Performs PCA on the whole dataframe\n",
    "def pca_transform(X_train, X_valid, X_test, \n",
    "                  features = [\"GarageArea\",\"YearRemodAdd\",\"TotalBsmtSF\",\"GrLivArea\"], \n",
    "                  n_components = None):\n",
    "    \n",
    "    # Normalize based on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_valid_scaled = scaler.transform(X_valid[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    \n",
    "    # Create principal components\n",
    "    pca = PCA(n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    X_valid_pca = pca.transform(X_valid_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    X_valid_pca = pd.DataFrame(X_valid_pca, columns=component_names)\n",
    "    X_test_pca = pd.DataFrame(X_test_pca, columns=component_names)\n",
    "    \n",
    "    return X_train.join(X_pca), X_valid.join(X_valid_pca), X_test.join(X_test_pca)\n",
    "\n",
    "def test_pca_features():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)\n",
    "    \n",
    "    transforms = [partial(pca_transform, n_components = 1)]\n",
    "    print(\"\\nPCA (1 components):\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "   \n",
    "    \n",
    "    transforms = [partial(pca_transform, n_components = 2)]\n",
    "    print(\"\\nPCA (2 components):\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "   \n",
    "    \n",
    "    transforms = [partial(pca_transform, n_components = 3)]\n",
    "    print(\"\\nPCA (3 components):\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "   \n",
    "    \n",
    "test_pca_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024a056",
   "metadata": {},
   "source": [
    "# Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11bd5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossFoldEncoder:\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        self.encoder_ = encoder\n",
    "        self.kwargs_ = kwargs  # keyword arguments for the encoder\n",
    "        self.cv_ = KFold(n_splits=5)\n",
    "\n",
    "    # Fit an encoder on one split and transform the feature on the\n",
    "    # other. Iterating over the splits in all folds gives a complete\n",
    "    # transformation. We also now have one trained encoder on each\n",
    "    # fold.\n",
    "    def fit_transform(self, X, y, cols):\n",
    "        self.fitted_encoders_ = []\n",
    "        self.cols_ = cols\n",
    "        X_encoded = []\n",
    "        for idx_encode, idx_train in self.cv_.split(X):\n",
    "            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n",
    "            fitted_encoder.fit(\n",
    "                X.iloc[idx_encode, :], y.iloc[idx_encode],\n",
    "            )\n",
    "            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n",
    "            self.fitted_encoders_.append(fitted_encoder)\n",
    "        X_encoded = pd.concat(X_encoded)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n",
    "\n",
    "    # To transform the test data, average the encodings learned from\n",
    "    # each fold.\n",
    "    def transform(self, X):\n",
    "        from functools import reduce\n",
    "\n",
    "        X_encoded_list = []\n",
    "        for fitted_encoder in self.fitted_encoders_:\n",
    "            X_encoded = fitted_encoder.transform(X)\n",
    "            X_encoded_list.append(X_encoded[self.cols_])\n",
    "        X_encoded = reduce(\n",
    "            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n",
    "        ) / len(X_encoded_list)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9ba5e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Target Encoding ('Neighborhood'):\n",
      "\n",
      "XGBoost  (3-fold Avg): 17103.0649\n",
      "XGBoost  (3-fold Max): 17470.149 \t 1.792s\n",
      "LightGBM (3-fold Avg): 16613.4951\n",
      "LightGBM (3-fold Max): 17245.4656 \t 1.771s\n",
      "CatBoost (3-fold Avg): 14871.7069\n",
      "CatBoost (3-fold Max): 15294.9878 \t 9.506s\n",
      "\n",
      "Overall (Avg):         16196.089\n",
      "Overall (Max):         17103.0649\n"
     ]
    }
   ],
   "source": [
    "def encode_neighborhood(X_train, X_valid, X_test, y_train):\n",
    "    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n",
    "    X1_train = encoder.fit_transform(X_train, y_train, cols=[\"Neighborhood\"])\n",
    "    X1_valid = encoder.transform(X_valid)\n",
    "    X1_test = encoder.transform(X_test)\n",
    "        \n",
    "    return X_train.join(X1_train), X_valid.join(X1_valid), X_test.join(X1_test)\n",
    "\n",
    "def test_neighborhood_encoding():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX) \n",
    "    \n",
    "    transforms = [encode_neighborhood]\n",
    "    print(\"\\nTarget Encoding ('Neighborhood'):\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "   \n",
    "    \n",
    "test_neighborhood_encoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56b9f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Target Encoding ('SubClass'):\n",
      "\n",
      "XGBoost  (3-fold Avg): 18346.5279\n",
      "XGBoost  (3-fold Max): 19315.441 \t 2.036s\n",
      "LightGBM (3-fold Avg): 16895.9533\n",
      "LightGBM (3-fold Max): 17523.6162 \t 1.749s\n",
      "CatBoost (3-fold Avg): 15345.1554\n",
      "CatBoost (3-fold Max): 15833.9808 \t 9.932s\n",
      "\n",
      "Overall (Avg):         16862.5455\n",
      "Overall (Max):         18346.5279\n"
     ]
    }
   ],
   "source": [
    "def encode_subclass(X_train, X_valid, X_test, y_train):\n",
    "    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n",
    "    X1_train = encoder.fit_transform(X_train, y_train, cols=[\"MSSubClass\"])\n",
    "    X1_valid = encoder.transform(X_valid)\n",
    "    X1_test = encoder.transform(X_test)\n",
    "        \n",
    "    return X_train.join(X1_train), X_valid.join(X1_valid), X_test.join(X1_test)\n",
    "\n",
    "def test_subclass_encoding():\n",
    "    \n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)    \n",
    "    \n",
    "    transforms = [encode_subclass]\n",
    "    print(\"\\nTarget Encoding ('SubClass'):\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "   \n",
    "    \n",
    "test_subclass_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc56601d",
   "metadata": {},
   "source": [
    "# Test Strategies\n",
    "\n",
    "We pick all the strategies which resulted in significant gains on average, namely:\n",
    "\n",
    "1. Mathematical Transformations\n",
    "2. Group Transformation\n",
    "3. Target Encoding\n",
    "4. Drop Columns (Mutual Information)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5cc6cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline\n",
      "\n",
      "Overall (Avg):         16560.5445\n",
      "Overall (Max):         17750.9292\n",
      "\n",
      "Target Encoding ('SubClass'):\n",
      "\n",
      "XGBoost  (3-fold Avg): 17269.873\n",
      "XGBoost  (3-fold Max): 18108.4344 \t 3.101s\n",
      "LightGBM (3-fold Avg): 16629.224\n",
      "LightGBM (3-fold Max): 17103.4907 \t 2.86s\n",
      "CatBoost (3-fold Avg): 14472.6178\n",
      "CatBoost (3-fold Max): 14847.5023 \t 11.908s\n",
      "\n",
      "Overall (Avg):         16123.9049\n",
      "Overall (Max):         17269.873\n"
     ]
    }
   ],
   "source": [
    "def test_features():\n",
    "    print(\"\\nBaseline\\n\")\n",
    "    print(\"Overall (Avg):\".ljust(22), BASELINE_AVG)\n",
    "    print(\"Overall (Max):\".ljust(22), BASELINE_MAX)    \n",
    "    \n",
    "    transforms = [mathematical_transformations, group_transformation, encode_neighborhood, remove_uninformative]\n",
    "    print(\"\\nTarget Encoding ('SubClass'):\\n\")\n",
    "    xgb_avg, xgb_max = score_xgboost(transforms)\n",
    "    lgbm_avg, lgbm_max = score_lightgbm(transforms)\n",
    "    cat_avg, cat_max = score_catboost(transforms)\n",
    "    print(\"\\nOverall (Avg):\".ljust(23), round(np.mean([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "    print(\"Overall (Max):\".ljust(22),round(np.max([xgb_avg, lgbm_avg, cat_avg]), 4))\n",
    "\n",
    "test_features()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
