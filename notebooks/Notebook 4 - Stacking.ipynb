{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98601baa",
   "metadata": {},
   "source": [
    "# Model Stacking\n",
    "\n",
    "In the previous notebook we used optuna to find hyperparameters which minimized the average mean absolute error on the training set using 3-fold cross-validation.\n",
    "\n",
    "In this notebook we will generate predictions using the three optimized models and then stack the models to see if we can get any improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cce9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables for testing changes to this notebook quickly\n",
    "FOLD_SEED = 0\n",
    "NUM_FOLDS = 3\n",
    "EARLY_STOP = 50\n",
    "TRIALS = 100\n",
    "SUBMIT = False\n",
    "STACK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e21c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from functools import partial \n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from category_encoders import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "# Feature Engineering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Models\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "import optuna\n",
    "from optuna.samplers import GridSampler\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "\n",
    "# Mute warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68721fd8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db24ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv(\"../data/train.csv\")\n",
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# Remove rows with missing target\n",
    "train.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "\n",
    "# Clean data, static transformations\n",
    "def clean_data(*data):\n",
    "    for df in data:\n",
    "        # fix typos to match documentation\n",
    "        df['MSZoning'] =  df['MSZoning'].replace({'C (all)': 'C'})\n",
    "        df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\":\"BrkComm\",\"Wd Shng\": \"WdShing\"})\n",
    "        df['Neighborhood'] = df['Neighborhood'].replace({'NAmes':'Names'})\n",
    "\n",
    "        # Some values of GarageYrBlt are corrupt, replace them with YearBuilt\n",
    "        df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n",
    "        \n",
    "        # optional feature: A - agriculture, C - commercial, R - residential, I - industrial\n",
    "        df[\"MSClass\"] = df['MSZoning'].map({'A': 'A','C': 'C',\"FV\": 'R','I': 'I',\n",
    "                                            \"RH\": 'R',\"RL\": 'R',\"RP\": 'R',\"RM\": 'R', np.nan:np.nan})\n",
    "    return data\n",
    "    \n",
    "train, test = clean_data(train, test)\n",
    "\n",
    "# List of categorical/numerical columns\n",
    "columns = [col for col in test.columns if col not in [\"Id\",\"MSClass\"]]\n",
    "object_cols = [col for col in columns if train[col].dtype == \"object\"]\n",
    "number_cols = [col for col in columns if train[col].dtype != \"object\"]\n",
    "\n",
    "# Define bins\n",
    "binner = KBinsDiscretizer(n_bins = 45, encode = 'ordinal', strategy = 'uniform')\n",
    "y_bins = binner.fit_transform(pd.DataFrame(data=train['SalePrice']))\n",
    "\n",
    "# Define folds\n",
    "train[\"kfold\"] = -1\n",
    "kf = StratifiedKFold(NUM_FOLDS, shuffle = True, random_state = FOLD_SEED) \n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(train, y_bins)):\n",
    "    train.loc[valid_idx,\"kfold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fba29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, X_valid, X_test):\n",
    "    \n",
    "    # 1. impute numerical data\n",
    "    columns = [col for col in X_train.columns if X_train[col].dtype != \"object\"]\n",
    "    if columns:\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_train[columns] = imputer.fit_transform(X_train[columns])\n",
    "        X_valid[columns] = imputer.transform(X_valid[columns])\n",
    "        X_test[columns] = imputer.transform(X_test[columns])\n",
    "    \n",
    "    # 2. impute categorical data\n",
    "    columns = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "    if columns:\n",
    "        imputer = SimpleImputer(strategy='constant', fill_value = 'None')\n",
    "        X_train[columns] = imputer.fit_transform(X_train[columns])\n",
    "        X_valid[columns] = imputer.transform(X_valid[columns])\n",
    "        X_test[columns] = imputer.transform(X_test[columns])\n",
    "    \n",
    "    # 3. encode 1-10 ratings\n",
    "    cols = [\"OverallQual\",\"OverallCond\"]\n",
    "    cols = [x for x in cols if x in X_train.columns]\n",
    "    ratings = {float(a):b for b,a in enumerate(range(1,11))}\n",
    "    mapping = [{'col':x, 'mapping': ratings} for x in cols]\n",
    "    \n",
    "    encoder = OrdinalEncoder(cols = cols, mapping = mapping)\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    # 4. encode Poor, Fair, Avg, Good, Ex ratings\n",
    "    cols = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"HeatingQC\", \"KitchenQual\",\"FireplaceQu\",\"GarageQual\",\"GarageCond\",'PoolQC']\n",
    "    cols = [x for x in cols if x in X_train.columns]\n",
    "    ratings = {\"Po\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4}\n",
    "    mapping = [{'col':x, 'mapping': ratings} for x in cols]\n",
    "    \n",
    "    encoder = OrdinalEncoder(cols = cols, mapping = mapping)\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    # 5. encode remaining ordinal data\n",
    "    cols = [\"LotShape\",\"LandSlope\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\n",
    "    \"Functional\",\"GarageFinish\",\"PavedDrive\",\"Utilities\",\"CentralAir\",\"Electrical\",\n",
    "    \"Fence\"]\n",
    "    cols = [x for x in cols if x in X_train.columns]\n",
    "    mapping = [{'col':\"LotShape\",\n",
    "                'mapping': {\"Reg\":0, \"IR1\":1, \"IR2\":2, \"IR3\":3}},\n",
    "               {'col':\"LandSlope\",\n",
    "                'mapping': {\"Sev\":0, \"Mod\":1, \"Gtl\":2}},\n",
    "               {'col':\"BsmtExposure\",\n",
    "                'mapping': {\"No\":0, \"Mn\":1, \"Av\":2, \"Gd\":3}},\n",
    "               {'col':\"BsmtFinType1\",\n",
    "                'mapping': {\"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}},\n",
    "               {'col':\"BsmtFinType2\",\n",
    "                'mapping': {\"Unf\":0, \"LwQ\":1, \"Rec\":2, \"BLQ\":3, \"ALQ\":4, \"GLQ\":5}},\n",
    "               {'col':\"Functional\",\n",
    "                'mapping': {\"Sal\":0, \"Sev\":1, \"Maj1\":2, \"Maj2\":3, \"Mod\":4, \"Min2\":5, \"Min1\":6, \"Typ\":7}},\n",
    "               {'col':\"GarageFinish\",\n",
    "                'mapping': {\"Unf\":0, \"RFn\":1, \"Fin\":2}},\n",
    "               {'col':\"PavedDrive\",\n",
    "                'mapping': {\"N\":0, \"P\":1, \"Y\":2}},\n",
    "               {'col':\"Utilities\",\n",
    "                'mapping': {\"NoSeWa\":0, \"NoSewr\":1, \"AllPub\":2}},\n",
    "               {'col':\"CentralAir\",\n",
    "                'mapping': {\"N\":0, \"Y\":1}},\n",
    "               {'col':\"Electrical\",\n",
    "                'mapping': {\"Mix\":0, \"FuseP\":1, \"FuseF\":2, \"FuseA\":3, \"SBrkr\":4}},\n",
    "               {'col':\"Fence\",\n",
    "                'mapping': {\"MnWw\":0, \"GdWo\":1, \"MnPrv\":2, \"GdPrv\":3}}]\n",
    "    mapping = [x for x in mapping if x['col'] in X_train.columns]\n",
    "    \n",
    "    encoder = OrdinalEncoder(cols = cols, mapping = mapping)\n",
    "    X_train = encoder.fit_transform(X_train)\n",
    "    X_valid = encoder.transform(X_valid)\n",
    "    X_test = encoder.transform(X_test)\n",
    "    \n",
    "    # Determine cardinality of remaining categorical data\n",
    "    columns = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
    "    high_cols = [col for col in columns if X_train[col].nunique() >= 10]\n",
    "    low_cols = [col for col in columns if X_train[col].nunique() < 10]\n",
    "    \n",
    "    # 6. ordinal encode high cardinality data\n",
    "    if high_cols:\n",
    "        encoder = OrdinalEncoder(cols = high_cols)\n",
    "        X_train = encoder.fit_transform(X_train)\n",
    "        X_valid = encoder.transform(X_valid)\n",
    "        X_test = encoder.transform(X_test)\n",
    "    \n",
    "    # 7. one-hot encode low cardinality data\n",
    "    if low_cols:\n",
    "        encoder = OneHotEncoder(cols = low_cols, use_cat_names = True)\n",
    "        X_train = encoder.fit_transform(X_train)\n",
    "        X_valid = encoder.transform(X_valid)\n",
    "        X_test = encoder.transform(X_test)\n",
    "        \n",
    "    return X_train, X_valid, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5456fa",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3e40129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mathematical_transformations(X_train, X_valid, X_test):\n",
    "    \n",
    "    X_train[\"LivLotRatio\"] = X_train[\"GrLivArea\"] / X_train[\"LotArea\"]\n",
    "    X_valid[\"LivLotRatio\"] = X_valid[\"GrLivArea\"] / X_valid[\"LotArea\"]\n",
    "    X_test[\"LivLotRatio\"] = X_test[\"GrLivArea\"] / X_test[\"LotArea\"]\n",
    "    \n",
    "    X_train[\"Spaciousness\"] = (X_train[\"1stFlrSF\"]+X_train[\"2ndFlrSF\"]) / X_train[\"TotRmsAbvGrd\"]\n",
    "    X_valid[\"Spaciousness\"] = (X_valid[\"1stFlrSF\"]+X_valid[\"2ndFlrSF\"]) / X_valid[\"TotRmsAbvGrd\"]\n",
    "    X_test[\"Spaciousness\"] = (X_test[\"1stFlrSF\"]+X_test[\"2ndFlrSF\"]) / X_test[\"TotRmsAbvGrd\"]\n",
    "    \n",
    "    X_train[\"TotalOutsideSF\"] = X_train[\"WoodDeckSF\"] + X_train[\"OpenPorchSF\"] + X_train[\"EnclosedPorch\"] + X_train[\"3SsnPorch\"] + X_train[\"ScreenPorch\"]\n",
    "    X_valid[\"TotalOutsideSF\"] = X_valid[\"WoodDeckSF\"] + X_valid[\"OpenPorchSF\"] + X_valid[\"EnclosedPorch\"] + X_valid[\"3SsnPorch\"] + X_valid[\"ScreenPorch\"]\n",
    "    X_test[\"TotalOutsideSF\"] = X_test[\"WoodDeckSF\"] + X_test[\"OpenPorchSF\"] + X_test[\"EnclosedPorch\"] + X_test[\"3SsnPorch\"] + X_test[\"ScreenPorch\"]\n",
    "    \n",
    "    X_train['TotalLot'] = X_train['LotFrontage'] + X_train['LotArea']\n",
    "    X_valid['TotalLot'] = X_valid['LotFrontage'] + X_valid['LotArea']\n",
    "    X_test['TotalLot'] = X_test['LotFrontage'] + X_test['LotArea']\n",
    "    \n",
    "    X_train['TotalBsmtFin'] = X_train['BsmtFinSF1'] + X_train['BsmtFinSF2']\n",
    "    X_valid['TotalBsmtFin'] = X_valid['BsmtFinSF1'] + X_valid['BsmtFinSF2']\n",
    "    X_test['TotalBsmtFin'] = X_test['BsmtFinSF1'] + X_test['BsmtFinSF2']\n",
    "    \n",
    "    X_train['TotalSF'] = X_train['TotalBsmtSF'] + X_train['2ndFlrSF'] + X_train['1stFlrSF']\n",
    "    X_valid['TotalSF'] = X_valid['TotalBsmtSF'] + X_valid['2ndFlrSF'] + X_valid['1stFlrSF']\n",
    "    X_test['TotalSF'] = X_test['TotalBsmtSF'] + X_test['2ndFlrSF'] + X_test['1stFlrSF']\n",
    "    \n",
    "    X_train['TotalBath'] = X_train['FullBath'] + X_train['HalfBath'] * 0.5 + X_train['BsmtFullBath'] + X_train['BsmtHalfBath'] * 0.5\n",
    "    X_valid['TotalBath'] = X_valid['FullBath'] + X_valid['HalfBath'] * 0.5 + X_valid['BsmtFullBath'] + X_valid['BsmtHalfBath'] * 0.5\n",
    "    X_test['TotalBath'] = X_test['FullBath'] + X_test['HalfBath'] * 0.5 + X_test['BsmtFullBath'] + X_test['BsmtHalfBath'] * 0.5\n",
    "    \n",
    "    X_train['TotalPorch'] = X_train['OpenPorchSF'] + X_train['EnclosedPorch'] + X_train['ScreenPorch'] + X_train['WoodDeckSF']\n",
    "    X_valid['TotalPorch'] = X_valid['OpenPorchSF'] + X_valid['EnclosedPorch'] + X_valid['ScreenPorch'] + X_valid['WoodDeckSF']\n",
    "    X_test['TotalPorch'] = X_test['OpenPorchSF'] + X_test['EnclosedPorch'] + X_test['ScreenPorch'] + X_test['WoodDeckSF']\n",
    "    \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def count_porch_types(X_train, X_valid, X_test):\n",
    "    \n",
    "    X_train[\"PorchTypes\"] = X_train[[\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]].gt(0).sum(axis=1)\n",
    "    X_valid[\"PorchTypes\"] = X_valid[[\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]].gt(0).sum(axis=1)\n",
    "    X_test[\"PorchTypes\"] = X_test[[\"WoodDeckSF\",\"OpenPorchSF\",\"EnclosedPorch\",\"3SsnPorch\",\"ScreenPorch\"]].gt(0).sum(axis=1)\n",
    "        \n",
    "    return X_train, X_valid, X_test\n",
    "\n",
    "def generate_cluster_distances(X_train, X_valid, X_test, name = \"Area\", features = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF','GrLivArea']):\n",
    "    \n",
    "    # 1. normalize based on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_valid_scaled = scaler.transform(X_valid[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    \n",
    "    # 2. generate cluster distances (use transform)\n",
    "    kmeans = KMeans(n_clusters = 10, n_init = 10, random_state=0)\n",
    "    X_cd = kmeans.fit_transform(X_scaled)\n",
    "    X_valid_cd = kmeans.transform(X_valid_scaled)\n",
    "    X_test_cd = kmeans.transform(X_test_scaled)\n",
    "    \n",
    "    # 3. column labels\n",
    "    X_cd = pd.DataFrame(X_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_cd.shape[1])])\n",
    "    X_valid_cd = pd.DataFrame(X_valid_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_valid_cd.shape[1])])\n",
    "    X_test_cd = pd.DataFrame(X_test_cd, columns=[name + \"_Centroid_\" + str(i) for i in range(X_test_cd.shape[1])])    \n",
    "    \n",
    "    return X_train.join(X_cd), X_valid.join(X_valid_cd), X_test.join(X_test_cd)\n",
    "\n",
    "def pca_transform(X_train, X_valid, X_test, \n",
    "                  features = [\"GarageArea\",\"YearRemodAdd\",\"TotalBsmtSF\",\"GrLivArea\"], \n",
    "                  n_components = 2):\n",
    "    \n",
    "    # Normalize based on training data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train[features])\n",
    "    X_valid_scaled = scaler.transform(X_valid[features])\n",
    "    X_test_scaled = scaler.transform(X_test[features])\n",
    "    \n",
    "    # Create principal components\n",
    "    pca = PCA(n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    X_valid_pca = pca.transform(X_valid_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n",
    "    X_pca = pd.DataFrame(X_pca, columns=component_names)\n",
    "    X_valid_pca = pd.DataFrame(X_valid_pca, columns=component_names)\n",
    "    X_test_pca = pd.DataFrame(X_test_pca, columns=component_names)\n",
    "    \n",
    "    return X_train.join(X_pca), X_valid.join(X_valid_pca), X_test.join(X_test_pca)\n",
    "\n",
    "class CrossFoldEncoder:\n",
    "    def __init__(self, encoder, **kwargs):\n",
    "        self.encoder_ = encoder\n",
    "        self.kwargs_ = kwargs  # keyword arguments for the encoder\n",
    "        self.cv_ = KFold(n_splits=5)\n",
    "\n",
    "    # Fit an encoder on one split and transform the feature on the\n",
    "    # other. Iterating over the splits in all folds gives a complete\n",
    "    # transformation. We also now have one trained encoder on each\n",
    "    # fold.\n",
    "    def fit_transform(self, X, y, cols):\n",
    "        self.fitted_encoders_ = []\n",
    "        self.cols_ = cols\n",
    "        X_encoded = []\n",
    "        for idx_encode, idx_train in self.cv_.split(X):\n",
    "            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n",
    "            fitted_encoder.fit(\n",
    "                X.iloc[idx_encode, :], y.iloc[idx_encode],\n",
    "            )\n",
    "            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n",
    "            self.fitted_encoders_.append(fitted_encoder)\n",
    "        X_encoded = pd.concat(X_encoded)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n",
    "\n",
    "    # To transform the test data, average the encodings learned from\n",
    "    # each fold.\n",
    "    def transform(self, X):\n",
    "        from functools import reduce\n",
    "\n",
    "        X_encoded_list = []\n",
    "        for fitted_encoder in self.fitted_encoders_:\n",
    "            X_encoded = fitted_encoder.transform(X)\n",
    "            X_encoded_list.append(X_encoded[self.cols_])\n",
    "        X_encoded = reduce(\n",
    "            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n",
    "        ) / len(X_encoded_list)\n",
    "        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n",
    "        return X_encoded\n",
    "    \n",
    "def encode_neighborhood(X_train, X_valid, X_test, y_train):\n",
    "    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n",
    "    X1_train = encoder.fit_transform(X_train, y_train, cols=[\"Neighborhood\"])\n",
    "    X1_valid = encoder.transform(X_valid)\n",
    "    X1_test = encoder.transform(X_test)\n",
    "        \n",
    "    return X_train.join(X1_train), X_valid.join(X1_valid), X_test.join(X1_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d068cc",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0edb3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = [\n",
    "    {'learning_rate': 0.0103, 'max_depth': 3, \n",
    "     'min_child_weight': 0.0608, 'colsample_bytree': 0.133, \n",
    "     'subsample': 0.6866000000000001, 'reg_lambda': 0.0806, \n",
    "     'reg_alpha': 0.1}, \n",
    "    {'learning_rate': 0.0098, 'max_depth': 3, \n",
    "     'min_child_weight': 0.6463, 'colsample_bytree': 0.14620000000000002, \n",
    "     'subsample': 0.6332, 'reg_lambda': 0.066, 'reg_alpha': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c62ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0  Fold 0 (MAE): 14307.284049345482\n",
      "Model 0  Fold 1 (MAE): 13135.857554222279\n",
      "Model 0  Fold 2 (MAE): 14317.888020833334\n",
      "Model 0 Average (MAE): 13920.3432081337\n",
      "Model 0 Worst (MAE): 14317.888020833334\n",
      "Model 1  Fold 0 (MAE): 14285.347343429157\n",
      "Model 1  Fold 1 (MAE): 13605.928380069301\n",
      "Model 1  Fold 2 (MAE): 14317.391042952675\n",
      "Model 1 Average (MAE): 14069.555588817044\n",
      "Model 1 Worst (MAE): 14317.391042952675\n"
     ]
    }
   ],
   "source": [
    "out_of_fold = pd.DataFrame({\"XGB\"+str(i): np.zeros((train.shape[0],)) for i in range(len(xgb_params))})\n",
    "out_of_fold['kfold'] = train.kfold\n",
    "predictions = pd.DataFrame({\"XGB\"+str(i): np.zeros((test.shape[0],)) for i in range(len(xgb_params))})\n",
    "\n",
    "for i, params in enumerate(xgb_params):\n",
    "    X = train.copy()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    transforms = [preprocessing, mathematical_transformations,\n",
    "                  count_porch_types, generate_cluster_distances,\n",
    "                  pca_transform, encode_neighborhood]\n",
    "\n",
    "    for j in range(NUM_FOLDS):\n",
    "        X_train = X[X.kfold != j][columns].reset_index(drop=True)\n",
    "        X_valid = X[X.kfold == j][columns].reset_index(drop=True)\n",
    "        y_train = X[X.kfold != j]['SalePrice'].reset_index(drop=True)\n",
    "        y_valid = X[X.kfold == j]['SalePrice'].reset_index(drop=True)\n",
    "        X_test = test[columns].copy()\n",
    "\n",
    "# loop for applying the transformations\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test, y_train = y_train)\n",
    "            except:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test)\n",
    "        \n",
    "        model = XGBRegressor(**{**{'random_state':0, \n",
    "                                   'n_estimators': 3000},\n",
    "                                **params})\n",
    "\n",
    "        model.fit(X_train, y_train,\n",
    "                  verbose = False,\n",
    "                  eval_set = [(X_valid, y_valid)],\n",
    "                  eval_metric = \"mae\",\n",
    "                  early_stopping_rounds = EARLY_STOP)\n",
    "\n",
    "        predictions[\"XGB\"+str(i)] += model.predict(X_test) / NUM_FOLDS \n",
    "        preds_valid = model.predict(X_valid)\n",
    "        scores[j] = mean_absolute_error(y_valid, preds_valid)\n",
    "        out_of_fold[\"XGB\"+str(i)][X.kfold == j] = preds_valid\n",
    "        print(\"Model\", i ,\" Fold\",j ,\"(MAE):\", scores[j])\n",
    "\n",
    "    print(\"Model\", i, \"Average (MAE):\", scores.mean())\n",
    "    print(\"Model\", i, \"Worst (MAE):\", scores.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971a508",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d80548",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_params = [\n",
    "    {'max_depth': 4,'num_leaves': 11, 'min_child_samples': 3,\n",
    "     'learning_rate': 0.0153, 'min_child_weight': 0.89, \n",
    "     'colsample_bytree': 0.215, 'subsample': 0.613, \n",
    "     'reg_lambda': 0.2906, 'reg_alpha': 88.21}, \n",
    "    {'learning_rate': 0.020700000000000003, 'max_depth': 3, \n",
    "     'num_leaves': 8, 'min_child_samples': 2, \n",
    "     'min_child_weight': 1.3800000000000001, \n",
    "     'colsample_bytree': 0.12890000000000001, \n",
    "     'subsample': 0.6342, 'reg_lambda': 1.589, 'reg_alpha': 86.924}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae782cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0  Fold 0 (MAE): 14747.866185848075\n",
      "Model 0  Fold 1 (MAE): 13717.18127967525\n",
      "Model 0  Fold 2 (MAE): 14469.271574308286\n",
      "Model 0 Average (MAE): 14311.439679943871\n",
      "Model 0 Worst (MAE): 14747.866185848075\n",
      "Model 1  Fold 0 (MAE): 14611.328682975776\n",
      "Model 1  Fold 1 (MAE): 13976.570776280563\n",
      "Model 1  Fold 2 (MAE): 14461.357889782365\n",
      "Model 1 Average (MAE): 14349.752449679567\n",
      "Model 1 Worst (MAE): 14611.328682975776\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lgbm_params)):\n",
    "    out_of_fold[\"LGBM\"+str(i)] = np.zeros((train.shape[0],))\n",
    "    predictions[\"LGBM\"+str(i)] = np.zeros((test.shape[0],))\n",
    "\n",
    "for i, params in enumerate(lgbm_params):\n",
    "    X = train.copy()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    transforms = [preprocessing, mathematical_transformations,\n",
    "                  count_porch_types, generate_cluster_distances,\n",
    "                  pca_transform, encode_neighborhood]\n",
    "\n",
    "    for j in range(NUM_FOLDS):\n",
    "        X_train = X[X.kfold != j][columns].reset_index(drop=True)\n",
    "        X_valid = X[X.kfold == j][columns].reset_index(drop=True)\n",
    "        y_train = X[X.kfold != j]['SalePrice'].reset_index(drop=True)\n",
    "        y_valid = X[X.kfold == j]['SalePrice'].reset_index(drop=True)\n",
    "        X_test = test[columns].copy()\n",
    "\n",
    "# loop for applying the transformations\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test, y_train = y_train)\n",
    "            except:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test)\n",
    "        \n",
    "        cat_cols = [x for x in X_train.columns if x in object_cols]\n",
    "\n",
    "        model = LGBMRegressor(**{**{'random_state':0, 'n_estimators': 4000, \n",
    "                                    'max_depth': 4,'num_leaves': 11, \n",
    "                                    'min_child_samples': 3,},\n",
    "                                 **params})\n",
    "        model.fit(X_train, y_train,\n",
    "                  verbose = False,\n",
    "                  eval_set = [(X_valid, y_valid)],\n",
    "                  eval_metric = \"mae\",\n",
    "                  categorical_feature = cat_cols,\n",
    "                  early_stopping_rounds = EARLY_STOP)\n",
    "\n",
    "        predictions[\"LGBM\"+str(i)] += model.predict(X_test) / NUM_FOLDS \n",
    "        preds_valid = model.predict(X_valid)\n",
    "        scores[j] = mean_absolute_error(y_valid, preds_valid)\n",
    "        out_of_fold[\"LGBM\"+str(i)][X.kfold == j] = preds_valid\n",
    "        print(\"Model\", i ,\" Fold\",j ,\"(MAE):\", scores[j])\n",
    "\n",
    "    print(\"Model\", i, \"Average (MAE):\", scores.mean())\n",
    "    print(\"Model\", i, \"Worst (MAE):\", scores.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237581d",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529086ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_params = [\n",
    "    {'learning_rate': 0.0147, 'subsample': 0.812, 'reg_lambda': 0.972},\n",
    "    {'learning_rate': 0.0227, 'subsample': 0.749, 'reg_lambda': 1.463}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf6fed6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0  Fold 0 (MAE): 14172.634905996987\n",
      "Model 0  Fold 1 (MAE): 12774.724306184993\n",
      "Model 0  Fold 2 (MAE): 14312.381667259804\n",
      "Model 0 Average (MAE): 13753.246959813929\n",
      "Model 0 Worst (MAE): 14312.381667259804\n",
      "Model 1  Fold 0 (MAE): 14260.626857891964\n",
      "Model 1  Fold 1 (MAE): 13018.354806356869\n",
      "Model 1  Fold 2 (MAE): 14292.017565886106\n",
      "Model 1 Average (MAE): 13856.999743378314\n",
      "Model 1 Worst (MAE): 14292.017565886106\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cat_params)):\n",
    "    out_of_fold[\"CAT\"+str(i)] = np.zeros((train.shape[0],))\n",
    "    predictions[\"CAT\"+str(i)] = np.zeros((test.shape[0],))\n",
    "    \n",
    "for i, params in enumerate(cat_params):\n",
    "    X = train.copy()\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    transforms = [preprocessing, mathematical_transformations,\n",
    "                  count_porch_types, generate_cluster_distances,\n",
    "                  pca_transform, encode_neighborhood]\n",
    "\n",
    "    for j in range(NUM_FOLDS):\n",
    "        X_train = X[X.kfold != j][columns].reset_index(drop=True)\n",
    "        X_valid = X[X.kfold == j][columns].reset_index(drop=True)\n",
    "        y_train = X[X.kfold != j]['SalePrice'].reset_index(drop=True)\n",
    "        y_valid = X[X.kfold == j]['SalePrice'].reset_index(drop=True)\n",
    "        X_test = test[columns].copy()\n",
    "\n",
    "# loop for applying the transformations\n",
    "        for transform in transforms:\n",
    "            try:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test, y_train = y_train)\n",
    "            except:\n",
    "                X_train, X_valid, X_test = transform(X_train, X_valid, X_test)\n",
    "        \n",
    "        model = CatBoostRegressor(**{**{'random_state':0, \n",
    "                                        'n_estimators': 3000,\n",
    "                                        'eval_metric':\"MAE\",\n",
    "                                        'early_stopping_rounds': EARLY_STOP,\n",
    "                                        'verbose': False}, \n",
    "                                     **params})\n",
    "        model.fit(X_train, y_train,\n",
    "                  eval_set = (X_valid, y_valid),\n",
    "                  use_best_model=True)\n",
    "\n",
    "        predictions[\"CAT\"+str(i)] += model.predict(X_test) / NUM_FOLDS \n",
    "        preds_valid = model.predict(X_valid)\n",
    "        scores[j] = mean_absolute_error(y_valid, preds_valid)\n",
    "        out_of_fold[\"CAT\"+str(i)][X.kfold == j] = preds_valid\n",
    "        print(\"Model\", i ,\" Fold\",j ,\"(MAE):\", scores[j])\n",
    "\n",
    "    print(\"Model\", i, \"Average (MAE):\", scores.mean())\n",
    "    print(\"Model\", i, \"Worst (MAE):\", scores.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f706956",
   "metadata": {},
   "source": [
    "# Save Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd02b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in predictions.columns:\n",
    "    if SUBMIT == False: break\n",
    "    output = pd.DataFrame({'Id': test.Id, 'SalePrice': predictions[col]})\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output.to_csv('../submissions/submission_'+col+'_'+timestr+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739992cc",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9af08774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking(stack_model, submit = False, fit_params = {}):\n",
    "    preds = np.zeros((test.shape[0],))\n",
    "    scores = np.zeros(NUM_FOLDS)\n",
    "    \n",
    "    for j in range(NUM_FOLDS):\n",
    "        X_train = out_of_fold[X.kfold != j].drop('kfold', axis = 1)\n",
    "        X_valid = out_of_fold[X.kfold == j].drop('kfold', axis = 1)\n",
    "        y_train = train['SalePrice'][X.kfold != j].copy()\n",
    "        y_valid = train['SalePrice'][X.kfold == j].copy()\n",
    "        X_test = predictions.copy()\n",
    "\n",
    "        model = clone(stack_model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        preds += model.predict(X_test) / NUM_FOLDS \n",
    "        preds_valid = model.predict(X_valid)\n",
    "        scores[j] = mean_absolute_error(y_valid, preds_valid)\n",
    "        print(\"Fold\", j ,\"(MAE):\", scores[j])\n",
    "\n",
    "    print(\"Avg (RMSE):\", round(scores.mean(),6))\n",
    "    print(\"Max (RMSE):\", round(scores.max(),6))\n",
    "\n",
    "    if submit:\n",
    "        output = pd.DataFrame({'Id': test.Id,'SalePrice': preds})\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        output.to_csv('../submissions/submission_stack_'+timestr+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1388ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 (MAE): 14698.520677439861\n",
      "Fold 1 (MAE): 13355.342565399234\n",
      "Fold 2 (MAE): 14643.656248590694\n",
      "Avg (RMSE): 14232.506497\n",
      "Max (RMSE): 14698.520677\n"
     ]
    }
   ],
   "source": [
    "stacking(stack_model = Lasso(), submit = STACK)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
